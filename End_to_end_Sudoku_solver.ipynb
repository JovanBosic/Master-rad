{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "End-to-end Sudoku solver",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1RtOzKr45MN2-xycqb-T9kMH3NCdg7Cw_",
      "authorship_tag": "ABX9TyPHrj0JM9FOxXyte0WKmE/q",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JovanBosic/Neural-nets-vs-SUDOKU-project/blob/main/End_to_end_Sudoku_solver.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bqRnGTkevWO"
      },
      "source": [
        "# End-to-end Sudoku solving using using neural nets ðŸ§©\n",
        "\n",
        "This notebook builds an end-to-end neural network using only NumPy, in order to solve Sudoku puzzle\n",
        "\n",
        "Sudoku is a popular number puzzle that requires you to fill blanks in a 9X9 grid with digits so that each column, each row, and each of the nine 3Ã—3 subgrids contains all of the digits from 1 to 9.\n",
        "\n",
        "## 1. Problem\n",
        "\n",
        "Find missing numbers and their places in unsolved sudoku.\n",
        "\n",
        "## 2. Data\n",
        "\n",
        "The data we are using is from Kaggle:\n",
        "\n",
        "https://www.kaggle.com/bryanpark/sudoku\n",
        "\n",
        "There is only one sv file, containing 1 000 000 unsolved and solved sudoku games.\n",
        "\n",
        "## 3. Evaluation\n",
        "\n",
        "If we can reach 85% accuracy at predicting numbers, we will pursue the project.\n",
        "\n",
        "## 4. Features\n",
        "\n",
        "A Sudoku puzzle is represented as a 9x9 Python numpy array. The blanks were replaced with 0's. \n",
        "\n",
        "There are to columns in the data:\n",
        "* quizzes\n",
        "* solutions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKW0AtFUOQwn"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVDU2sO0l5BI"
      },
      "source": [
        "### Get our workspace ready\n",
        "\n",
        "* Import NumPy âœ…\n",
        "* Import Pandasâœ…\n",
        "* Import Scikit-Learn âœ…\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MashyljZmc9a"
      },
      "source": [
        "# Import necessary tools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrVCi2DamtUN"
      },
      "source": [
        "### Load our Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xiXaABY-mtAH",
        "outputId": "c2c661f0-7830-43ee-d354-2d93d174530e"
      },
      "source": [
        "# Import data set\n",
        "path = \"drive/MyDrive/Colab/sudoku.csv\"\n",
        "df = pd.read_csv(path)\n",
        "df.head(), df.shape # Rows, Columns"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(                                             quizzes                                          solutions\n",
              " 0  0043002090050090010700600430060020871900074000...  8643712593258497619712658434361925871986574322...\n",
              " 1  0401000501070039605200080000000000170009068008...  3461792581875239645296483719658324174729168358...\n",
              " 2  6001203840084590720000060050002640300700800069...  6951273841384596727248369158512647392739815469...\n",
              " 3  4972000001004000050000160986203000403009000000...  4972583161864397252537164986293815473759641828...\n",
              " 4  0059103080094030600275001000300002010008200070...  4659123781894735623275681497386452919548216372...,\n",
              " (1000000, 2))"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QG0SEmvUv9gq",
        "outputId": "7d3cd0c6-94b8-42ec-a89d-d046c96892e5"
      },
      "source": [
        "# Are there any missing values?\n",
        "df.isna().sum()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "quizzes      0\n",
              "solutions    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuIY_1OQwuE_"
      },
      "source": [
        "X = df[\"quizzes\"]\n",
        "y = df[\"solutions\"]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PX2KwqSw7ho",
        "outputId": "0f59512f-e783-4403-cc47-cce1554b18d1"
      },
      "source": [
        "# Let's check the data\n",
        "print(X[:10])\n",
        "print(X.shape)\n",
        "print(X.dtype)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    0043002090050090010700600430060020871900074000...\n",
            "1    0401000501070039605200080000000000170009068008...\n",
            "2    6001203840084590720000060050002640300700800069...\n",
            "3    4972000001004000050000160986203000403009000000...\n",
            "4    0059103080094030600275001000300002010008200070...\n",
            "5    1000050073809000006000004808200010750407600200...\n",
            "6    0090654300070008006001080200030900025014039608...\n",
            "7    0000006577024001003500060005000200092103005000...\n",
            "8    5030701900000067500471906004000380009502003000...\n",
            "9    0607209080840030017001000659000080000710600000...\n",
            "Name: quizzes, dtype: object\n",
            "(1000000,)\n",
            "object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-NafrQuNW13"
      },
      "source": [
        "### Changing data shape and type"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bv-9qpdezSxr"
      },
      "source": [
        "Since we need every number separately, we have to:\n",
        "* Cast objects to numbers âœ…\n",
        "* `X.shape` should be (1000000, 81) âœ…"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e25KESqYft4V"
      },
      "source": [
        "def get_data_in_right_form(X, y): \n",
        "    '''\n",
        "    This function reshapes features and labels into matrix 1 000 000, 81, \n",
        "    and scale feature values between -0.5 and 0.5\n",
        "    '''\n",
        "    # Lists where wi will store new reshaped features and labels\n",
        "    X_new = []\n",
        "    y_new = []\n",
        "\n",
        "    for i in X:\n",
        "        # Reshaping our features into shape 1 000 000, 81\n",
        "        x = np.array([int(j) for j in i])\n",
        "        X_new.append(x)\n",
        "\n",
        "    X_new = np.array(X_new)\n",
        "    # Convert values of quizzes from 0-9 to -0.5-0.5 for better perfomances during training\n",
        "    X_new = X_new/9\n",
        "    X_new -= .5    \n",
        "    \n",
        "    for i in y:\n",
        "        # Reshaping our labels into shape 1 000 000, 81\n",
        "        x = np.array([int(j) for j in i])\n",
        "        y_new.append(x)   \n",
        "    \n",
        "    y_new = np.array(y_new)    \n",
        "    # Spliting data into train and test sets\n",
        "    x_train, x_test, y_train, y_test = train_test_split(X_new, y_new, test_size=0.35, random_state=42)\n",
        "    \n",
        "    return x_train, x_test, y_train, y_test"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpIdf6Qqbi9B"
      },
      "source": [
        "X, X_test, y, y_test = get_data_in_right_form(X, y)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdUAeSbRbxSw",
        "outputId": "172077a3-a70d-493b-e13e-1aec8d18adfe"
      },
      "source": [
        "print(X.shape)\n",
        "print(X_test.shape)\n",
        "print(y.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(650000, 81)\n",
            "(350000, 81)\n",
            "(650000, 81)\n",
            "(350000, 81)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxPsfhP6Nixr"
      },
      "source": [
        "### Making our labels easy to use"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9wvqh2vC4s_"
      },
      "source": [
        "Now we have to change our labels. We have to get 81 numbers for each position in the sudoku game, not just one, and we have a total of 9 classes for each number because a number can fall in a range of 1 to 9.\n",
        "\n",
        "To comply with this design, our network should output 81x9=729 numbers.\n",
        "\n",
        "In order to compare our `predicted labels`, and `true labels` we have to change our current labels shape. Since our current true  labels are in shape 1 000 000, 81 we will have to change this somehow to 1 000 000, 729. My idea was to every number in current label represent as 0 and 1 where the lenght will be 9. \n",
        "\n",
        "**For example:** \n",
        "* 1 == 100000000\n",
        "* 5 == 000010000...\n",
        "\n",
        "This wat we can easily compare `predicted labels` and `true labels`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8Oh3F7mcnbH"
      },
      "source": [
        "# Lets create a function from transforming our labels\n",
        "def label_transformer(y):\n",
        "    '''\n",
        "    This function reshapes labes from 1 000 000,81 into 1 000 000, 729 matix \n",
        "    by representing every number with 0 and 1\n",
        "    '''\n",
        "    zeroes = [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "    h = []\n",
        "    y_transform = []\n",
        "    for i in y:\n",
        "        for j in i:\n",
        "            zeroes[j-1] = 1\n",
        "            h += zeroes\n",
        "            zeroes = [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "        y_transform.append(h)\n",
        "        h = []\n",
        "\n",
        "    y_transform = np.array(y_transform)\n",
        "    return y_transform"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPan1AjpK4DH"
      },
      "source": [
        "y_transform = label_transformer(y)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59N8RsRjd5zV",
        "outputId": "87b5b503-848a-4e66-ee51-d955ba4720d1"
      },
      "source": [
        "print(y[1])\n",
        "print(y_transform[1])\n",
        "print(y_transform.shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 6 7 3 8 2 4 5 9 4 2 8 5 9 6 7 3 1 3 9 5 4 1 7 8 6 2 5 4 9 7 6 8 1 2 3 7\n",
            " 1 2 9 5 3 6 4 8 8 3 6 1 2 4 5 9 7 6 7 4 2 3 1 9 8 5 9 8 3 6 7 5 2 1 4 2 5\n",
            " 1 8 4 9 3 7 6]\n",
            "[1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
            " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0\n",
            " 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1\n",
            " 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
            " 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1\n",
            " 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
            " 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0\n",
            " 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
            " 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0\n",
            " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
            " 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0\n",
            " 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
            " 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0\n",
            " 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0\n",
            " 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
            " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
            " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0]\n",
            "(650000, 729)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctjgVekeeCd4"
      },
      "source": [
        "y_test_transform = label_transformer(y_test)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5g0LJ6_PeEaD",
        "outputId": "12369c57-46eb-4c5e-b696-cf24fb845952"
      },
      "source": [
        "print(y_test_transform[1])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
            " 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0\n",
            " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
            " 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0\n",
            " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0\n",
            " 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
            " 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0\n",
            " 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
            " 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0\n",
            " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
            " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0\n",
            " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0\n",
            " 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0\n",
            " 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNESV5vwM29k"
      },
      "source": [
        "Now we have got our data into training and test sets, it is time to build a machine learning model.\n",
        "\n",
        "We will train it (find the patterns) on the training set.\n",
        "\n",
        "And we will test it (use the patterns) on the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ft__2qQNslF"
      },
      "source": [
        "## 5. Building artificial neural network model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeLOs6TiOAf0"
      },
      "source": [
        "### Modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vf8C8UexCpwj"
      },
      "source": [
        "#### Dense Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGNT9tWACtyC"
      },
      "source": [
        "# Dense layer\n",
        "class Layer_Dense:\n",
        "\n",
        "    # Layer initialization\n",
        "    def __init__(self, n_inputs, n_neurons,\n",
        "                 weight_regularizer_l1=0, weight_regularizer_l2=0,\n",
        "                 bias_regularizer_l1=0, bias_regularizer_l2=0):\n",
        "        # Initialize weights and biases\n",
        "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
        "        self.biases = np.zeros((1, n_neurons))\n",
        "        # Set regularization strength\n",
        "        self.weight_regularizer_l1 = weight_regularizer_l1\n",
        "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
        "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
        "        self.bias_regularizer_l2 = bias_regularizer_l2\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs, training):\n",
        "        # Remember input values\n",
        "        self.inputs = inputs\n",
        "        # Calculate output values from inputs, weights and biases\n",
        "        self.output = np.dot(inputs, self.weights) + self.biases\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "        # Gradients on parameters\n",
        "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
        "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
        "\n",
        "\n",
        "        # Gradients on regularization\n",
        "        # L1 on weights\n",
        "        if self.weight_regularizer_l1 > 0:\n",
        "            dL1 = np.ones_like(self.weights)\n",
        "            dL1[self.weights < 0] = -1\n",
        "            self.dweights += self.weight_regularizer_l1 * dL1\n",
        "        # L2 on weights\n",
        "        if self.weight_regularizer_l2 > 0:\n",
        "            self.dweights += 2 * self.weight_regularizer_l2 * self.weights\n",
        "        # L1 on biases\n",
        "        if self.bias_regularizer_l1 > 0:\n",
        "            dL1 = np.ones_like(self.biases)\n",
        "            dL1[self.biases < 0] = -1\n",
        "            self.dbiases += self.bias_regularizer_l1 * dL1\n",
        "        # L2 on biases\n",
        "        if self.bias_regularizer_l2 > 0:\n",
        "            self.dbiases += 2 * self.bias_regularizer_l2 * self.biases\n",
        "\n",
        "        # Gradient on values\n",
        "        self.dinputs = np.dot(dvalues, self.weights.T)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JQZXNHCCy-k"
      },
      "source": [
        "#### Dropout Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1LJiuLJC4Mf"
      },
      "source": [
        "# Dropout\n",
        "class Layer_Dropout:\n",
        "\n",
        "    # Init\n",
        "    def __init__(self, rate):\n",
        "        # Store rate, we invert it as for example for dropout\n",
        "        # of 0.1 we need success rate of 0.9\n",
        "        self.rate = 1 - rate\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs, training):\n",
        "        # Save input values\n",
        "        self.inputs = inputs\n",
        "\n",
        "\n",
        "        # If not in the training mode - return values\n",
        "        if not training:\n",
        "            self.output = inputs.copy()\n",
        "            return\n",
        "\n",
        "        # Generate and save scaled mask\n",
        "        self.binary_mask = np.random.binomial(1, self.rate, size=inputs.shape) / self.rate\n",
        "        # Apply mask to output values\n",
        "        self.output = inputs * self.binary_mask\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "        # Gradient on values\n",
        "        self.dinputs = dvalues * self.binary_mask"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4q0N9B6vC5gI"
      },
      "source": [
        "#### Input Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlCzB6ifC-aj"
      },
      "source": [
        "# Input \"layer\"\n",
        "class Layer_Input:\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs, training):\n",
        "        self.output = inputs"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZo-9Ar0DBCb"
      },
      "source": [
        "#### ReLU Activation Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRwo5LabDIz-"
      },
      "source": [
        "# ReLU activation\n",
        "class Activation_ReLU:\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs, training):\n",
        "        # Remember input values\n",
        "        self.inputs = inputs\n",
        "        # Calculate output values from inputs\n",
        "        self.output = np.maximum(0, inputs)\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "        # Since we need to modify original variable,\n",
        "        # let's make a copy of values first\n",
        "        self.dinputs = dvalues.copy()\n",
        "\n",
        "        # Zero gradient where input values were negative\n",
        "        self.dinputs[self.inputs <= 0] = 0\n",
        "\n",
        "    # Calculate predictions for outputs\n",
        "    def predictions(self, outputs):\n",
        "        return outputs"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dZryl1XDLHO"
      },
      "source": [
        "#### Softmax Activation Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBbewxryDPGK"
      },
      "source": [
        "# Softmax activation\n",
        "def calculating_probabilities_for_softmax(inputs):\n",
        "    '''\n",
        "    This function iterates through every sample in batch, and sum exp values\n",
        "    of every 9 numbers in sample, after that it perform normalization where we devide \n",
        "    every 9 numbers with the sum of corresponding 9 numbers from group where the numbers \n",
        "    belong. Output is in a shape batch_lenght * 729 and goes further to the loss funtion.\n",
        "    '''\n",
        "    row, columns = inputs.shape\n",
        "    probabilities = np.array([])\n",
        "    counter = 0 \n",
        "\n",
        "    # Iterate through batch\n",
        "    while counter < row:\n",
        "        sample = inputs[counter, :]\n",
        "        sample = sample.reshape(-1, 9)\n",
        "\n",
        "        max = np.max(sample, axis=1,keepdims=True) #returns max of each row and keeps same dims\n",
        "        e_x = np.exp(sample-max) #subtracts each row with its max value\n",
        "        sum = np.sum(e_x, axis=1, keepdims=True) #returns sum of each row and keeps same dims\n",
        "        f_x = e_x / sum \n",
        "        sample_probabilities = f_x.reshape(1, -1)\n",
        "\n",
        "        counter += 1\n",
        "        probabilities = np.append(probabilities, sample_probabilities)\n",
        "    probabilities = probabilities.reshape(row, columns)  \n",
        "    return probabilities\n",
        "\n",
        "\n",
        "class Activation_Softmax:\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs, training):\n",
        "        # Remember input values\n",
        "        self.inputs = inputs\n",
        "\n",
        "        # Get probabilities\n",
        "        probabilities = calculating_probabilities_for_softmax(inputs) \n",
        "        self.output = probabilities\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "\n",
        "        # Create uninitialized array\n",
        "        self.dinputs = np.empty_like(dvalues)\n",
        "\n",
        "        # Enumerate outputs and gradients\n",
        "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
        "            # Flatten output array\n",
        "            single_output = single_output.reshape(-1, 1)\n",
        "            # Calculate Jacobian matrix of the output\n",
        "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
        "            # Calculate sample-wise gradient\n",
        "            # and add it to the array of sample gradients\n",
        "            self.dinputs[index] = np.dot(jacobian_matrix,\n",
        "                                         single_dvalues)\n",
        "\n",
        "    # Calculate predictions for outputs\n",
        "    def predictions(self, outputs):\n",
        "        '''\n",
        "        This function returns indexes of highest values for every group of numbers. \n",
        "        Output is in shape batch_lenght * 81 (for every 9 outputs one index)\n",
        "        '''\n",
        "        row, columns = outputs.shape\n",
        "        i = 0\n",
        "        counter = 0\n",
        "        argmax_array = np.array([])\n",
        "        sample_argmaxes = np.array([])\n",
        "\n",
        "        # Iterate through batches\n",
        "        while counter < row:\n",
        "            # Iterate through sample \n",
        "            for i in range(81):\n",
        "                argmax_for_groups_of_9 = np.argmax(outputs[counter, i*9:i*9+9]) + (i*9) \n",
        "                sample_argmaxes = np.append(sample_argmaxes,argmax_for_groups_of_9)\n",
        "\n",
        "            counter += 1\n",
        "            argmax_array = np.append(argmax_array, sample_argmaxes, axis=0)\n",
        "            sample_argmaxes = np.array([])\n",
        "        argmax_array = argmax_array.reshape(row,81)\n",
        "\n",
        "        return argmax_array\n"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixsLGJrqDWua"
      },
      "source": [
        "#### Adam Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuZD2djZDdom"
      },
      "source": [
        "# Adam optimizer\n",
        "class Optimizer_Adam:\n",
        "\n",
        "    # Initialize optimizer - set settings\n",
        "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7,\n",
        "                 beta_1=0.9, beta_2=0.999):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.current_learning_rate = learning_rate\n",
        "        self.decay = decay\n",
        "        self.iterations = 0\n",
        "        self.epsilon = epsilon\n",
        "        self.beta_1 = beta_1\n",
        "        self.beta_2 = beta_2\n",
        "\n",
        "    # Call once before any parameter updates\n",
        "    def pre_update_params(self):\n",
        "        if self.decay:\n",
        "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "    # Update parameters\n",
        "    def update_params(self, layer):\n",
        "\n",
        "        # If layer does not contain cache arrays,\n",
        "        # create them filled with zeros\n",
        "        if not hasattr(layer, 'weight_cache'):\n",
        "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
        "            layer.weight_cache = np.zeros_like(layer.weights)\n",
        "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
        "            layer.bias_cache = np.zeros_like(layer.biases)\n",
        "\n",
        "\n",
        "        # Update momentum  with current gradients\n",
        "        layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1 - self.beta_1) * layer.dweights\n",
        "        layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1 - self.beta_1) * layer.dbiases\n",
        "        # Get corrected momentum\n",
        "        # self.iteration is 0 at first pass\n",
        "        # and we need to start with 1 here\n",
        "        weight_momentums_corrected = layer.weight_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
        "        bias_momentums_corrected = layer.bias_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
        "        # Update cache with squared current gradients\n",
        "        layer.weight_cache = self.beta_2 * layer.weight_cache + (1 - self.beta_2) * layer.dweights**2\n",
        "        layer.bias_cache = self.beta_2 * layer.bias_cache + (1 - self.beta_2) * layer.dbiases**2\n",
        "        # Get corrected cache\n",
        "        weight_cache_corrected = layer.weight_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
        "        bias_cache_corrected = layer.bias_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
        "\n",
        "        # Vanilla SGD parameter update + normalization\n",
        "        # with square rooted cache\n",
        "        layer.weights += -self.current_learning_rate * weight_momentums_corrected / (np.sqrt(weight_cache_corrected) + self.epsilon)\n",
        "        layer.biases += -self.current_learning_rate * bias_momentums_corrected / (np.sqrt(bias_cache_corrected) + self.epsilon)\n",
        "\n",
        "    # Call once after any parameter updates\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OL-amQM5Dlvt"
      },
      "source": [
        "#### Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZS1t6CeDn_N"
      },
      "source": [
        "# Common loss class\n",
        "class Loss:\n",
        "\n",
        "    # Regularization loss calculation\n",
        "    def regularization_loss(self):\n",
        "\n",
        "        # 0 by default\n",
        "        regularization_loss = 0\n",
        "\n",
        "        # Calculate regularization loss\n",
        "        # iterate all trainable layers\n",
        "        for layer in self.trainable_layers:\n",
        "\n",
        "            # L1 regularization - weights\n",
        "            # calculate only when factor greater than 0\n",
        "            if layer.weight_regularizer_l1 > 0:\n",
        "                regularization_loss += layer.weight_regularizer_l1 * np.sum(np.abs(layer.weights))\n",
        "\n",
        "            # L2 regularization - weights\n",
        "            if layer.weight_regularizer_l2 > 0:\n",
        "                regularization_loss += layer.weight_regularizer_l2 * np.sum(layer.weights * layer.weights)\n",
        "\n",
        "            # L1 regularization - biases\n",
        "            # calculate only when factor greater than 0\n",
        "            if layer.bias_regularizer_l1 > 0:\n",
        "                regularization_loss += layer.bias_regularizer_l1 * np.sum(np.abs(layer.biases))\n",
        "\n",
        "            # L2 regularization - biases\n",
        "            if layer.bias_regularizer_l2 > 0:\n",
        "                regularization_loss += layer.bias_regularizer_l2 * np.sum(layer.biases * layer.biases)\n",
        "\n",
        "        return regularization_loss\n",
        "\n",
        "    # Set/remember trainable layers\n",
        "    def remember_trainable_layers(self, trainable_layers):\n",
        "        self.trainable_layers = trainable_layers\n",
        "\n",
        "\n",
        "    # Calculates the data and regularization losses\n",
        "    # given model output and ground truth values\n",
        "    def calculate(self, output, y, *, include_regularization=False):\n",
        "\n",
        "        # Calculate sample losses\n",
        "        sample_losses = self.forward(output, y)\n",
        "\n",
        "        # Calculate mean loss\n",
        "        data_loss = np.mean(sample_losses)\n",
        "\n",
        "        # Add accumulated sum of losses and sample count\n",
        "        self.accumulated_sum += np.sum(sample_losses)\n",
        "        self.accumulated_count += len(sample_losses)\n",
        "\n",
        "        # If just data loss - return it\n",
        "        if not include_regularization:\n",
        "            return data_loss\n",
        "\n",
        "        # Return the data and regularization losses\n",
        "        return data_loss, self.regularization_loss()\n",
        "\n",
        "    # Calculates accumulated loss\n",
        "    def calculate_accumulated(self, *, include_regularization=False):\n",
        "\n",
        "        # Calculate mean loss\n",
        "        data_loss = self.accumulated_sum / self.accumulated_count\n",
        "\n",
        "        # If just data loss - return it\n",
        "        if not include_regularization:\n",
        "            return data_loss\n",
        "\n",
        "        # Return the data and regularization losses\n",
        "        return data_loss, self.regularization_loss()\n",
        "\n",
        "    # Reset variables for accumulated loss\n",
        "    def new_pass(self):\n",
        "        self.accumulated_sum = 0\n",
        "        self.accumulated_count = 0\n",
        "\n",
        "\n",
        "# Cross-entropy loss\n",
        "class Loss_CategoricalCrossentropy(Loss):\n",
        "\n",
        "    def forward(self, y_pred, y_true):\n",
        "        \"\"\"\n",
        "        This function calculates loss for every place in sudoku separately and sum all losses at end\n",
        "        \"\"\"\n",
        "        # Clip data to prevent division by 0\n",
        "        # Clip both sides to not drag mean towards any value\n",
        "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
        "        row,columns = y_pred_clipped.shape\n",
        "        counter=0\n",
        "        negative_log_likelihoods = np.array([])\n",
        "        sample_loss_sum = 0\n",
        "\n",
        "        # Iterate through batch\n",
        "        while counter < row:\n",
        "            # Iterate through sample\n",
        "            for i in range(columns):\n",
        "                if y_true[counter,i] == 1:\n",
        "                    temp_loss = -y_true[counter,i] * np.log(y_pred_clipped[counter,i])\n",
        "                    sample_loss_sum = sample_loss_sum + temp_loss\n",
        "\n",
        "            counter += 1\n",
        "            negative_log_likelihoods = np.append(negative_log_likelihoods,sample_loss_sum)\n",
        "            sample_loss_sum = 0\n",
        "            #negative_log_likelihoods = negative_log_likelihoods / 81\n",
        "\n",
        "        return negative_log_likelihoods\n",
        "\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues, y_true):\n",
        "\n",
        "        # Number of samples\n",
        "        samples = len(dvalues)\n",
        "        # Number of labels in every sample\n",
        "        # We'll use the first sample to count them\n",
        "        labels = len(dvalues[0])\n",
        "\n",
        "        # If labels are sparse, turn them into one-hot vector\n",
        "        if len(y_true.shape) == 1:\n",
        "            y_true = np.eye(labels)[y_true]\n",
        "\n",
        "        # Calculate gradient\n",
        "        self.dinputs = -y_true / dvalues\n",
        "        # Normalize gradient\n",
        "        self.dinputs = self.dinputs / samples\n",
        "\n",
        "\n",
        "\n",
        "# Softmax classifier - combined Softmax activation\n",
        "# and cross-entropy loss for faster backward step\n",
        "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues, y_true):\n",
        "\n",
        "        # Number of samples\n",
        "        samples = len(dvalues)\n",
        "\n",
        "        # If labels are one-hot encoded,\n",
        "        # turn them into discrete values\n",
        "        if len(y_true.shape) == 2:\n",
        "            y_true = np.argmax(y_true, axis=1)\n",
        "\n",
        "        # Copy so we can safely modify\n",
        "        self.dinputs = dvalues.copy()\n",
        "        # Calculate gradient\n",
        "        self.dinputs[range(samples), y_true] -= 1\n",
        "        # Normalize gradient\n",
        "        self.dinputs = self.dinputs / samples\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEW7KtB0DsMI"
      },
      "source": [
        "#### Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcTfFUH2DuZo"
      },
      "source": [
        "# Common accuracy class\n",
        "class Accuracy:\n",
        "\n",
        "    # Calculates an accuracy\n",
        "    # given predictions and ground truth values\n",
        "    def calculate(self, predictions, y):\n",
        "\n",
        "        # Get comparison results\n",
        "        comparisons = self.compare(predictions, y)\n",
        "\n",
        "        # Calculate an accuracy\n",
        "        accuracy = np.mean(comparisons)\n",
        "        # print(accuracy, np.mean(comparisons))\n",
        "        \n",
        "\n",
        "        # Add accumulated sum of matching values and sample count\n",
        "        self.accumulated_sum += np.sum(comparisons)\n",
        "        self.accumulated_count += len(comparisons)\n",
        "\n",
        "        # Return accuracy\n",
        "        return accuracy\n",
        "\n",
        "    # Calculates accumulated accuracy\n",
        "    def calculate_accumulated(self):\n",
        "\n",
        "        # Calculate an accuracy\n",
        "        accuracy = self.accumulated_sum / self.accumulated_count\n",
        "\n",
        "        # Return the data and regularization losses\n",
        "        return accuracy\n",
        "\n",
        "    # Reset variables for accumulated accuracy\n",
        "    def new_pass(self):\n",
        "        self.accumulated_sum = 0\n",
        "        self.accumulated_count = 0\n",
        "\n",
        "\n",
        "\n",
        "# Accuracy calculation for classification model\n",
        "class Accuracy_Categorical(Accuracy):\n",
        "\n",
        "    def __init__(self, *, binary=False):\n",
        "        # Binary mode?\n",
        "        self.binary = binary\n",
        "\n",
        "    # No initialization is needed\n",
        "    def init(self, y):\n",
        "        pass\n",
        "\n",
        "    # Compares predictions to the ground truth values\n",
        "    def compare(self, predictions, y):\n",
        "        \"\"\"\n",
        "        This function compares predictions and labels. Since our predictions are in shape\n",
        "        batch size * 81 we need to change our labels. Labels will have size batch size * 81 after\n",
        "        we take the indexes of ones in labels. \n",
        "        \"\"\"\n",
        "        if not self.binary and len(y.shape) == 2:\n",
        "            row, columns = y.shape\n",
        "            i = 0\n",
        "            counter = 0\n",
        "            y_temp = np.array([])\n",
        "            temp_indexes = np.array([])\n",
        "\n",
        "            # Iterate through batches\n",
        "            while counter < row:\n",
        "                # Iterate through samples\n",
        "                for i in range(81):\n",
        "                    max_value_index = np.argmax(y[counter,i*9:i*9+9])+(9*i) \n",
        "                    temp_indexes = np.append(temp_indexes,max_value_index)\n",
        "                counter += 1\n",
        "                y_temp = np.append(y_temp, temp_indexes, axis=0)\n",
        "                temp_indexes = np.array([])\n",
        "            y_temp = y_temp.reshape(row,81)\n",
        "            \n",
        "        return y_temp==predictions"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uF-nWvTrDy_P"
      },
      "source": [
        "#### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_elhu3ywW1rd"
      },
      "source": [
        "# Model class\n",
        "class Model:\n",
        "\n",
        "    def __init__(self):\n",
        "        # Create a list of network objects\n",
        "        self.layers = []\n",
        "        # Softmax classifier's output object\n",
        "        self.softmax_classifier_output = None\n",
        "\n",
        "    # Add objects to the model\n",
        "    def add(self, layer):\n",
        "        self.layers.append(layer)\n",
        "\n",
        "\n",
        "    # Set loss, optimizer and accuracy\n",
        "    def set(self, *, loss, optimizer, accuracy):\n",
        "        self.loss = loss\n",
        "        self.optimizer = optimizer\n",
        "        self.accuracy = accuracy\n",
        "\n",
        "    # Finalize the model\n",
        "    def finalize(self):\n",
        "\n",
        "        # Create and set the input layer\n",
        "        self.input_layer = Layer_Input()\n",
        "\n",
        "        # Count all the objects\n",
        "        layer_count = len(self.layers)\n",
        "\n",
        "        # Initialize a list containing trainable layers:\n",
        "        self.trainable_layers = []\n",
        "\n",
        "        # Iterate the objects\n",
        "        for i in range(layer_count):\n",
        "\n",
        "            # If it's the first layer,\n",
        "            # the previous layer object is the input layer\n",
        "            if i == 0:\n",
        "                self.layers[i].prev = self.input_layer\n",
        "                self.layers[i].next = self.layers[i+1]\n",
        "\n",
        "            # All layers except for the first and the last\n",
        "            elif i < layer_count - 1:\n",
        "                self.layers[i].prev = self.layers[i-1]\n",
        "                self.layers[i].next = self.layers[i+1]\n",
        "\n",
        "            # The last layer - the next object is the loss\n",
        "            # Also let's save aside the reference to the last object\n",
        "            # whose output is the model's output\n",
        "            else:\n",
        "                self.layers[i].prev = self.layers[i-1]\n",
        "                self.layers[i].next = self.loss\n",
        "                self.output_layer_activation = self.layers[i]\n",
        "\n",
        "\n",
        "            # If layer contains an attribute called \"weights\",\n",
        "            # it's a trainable layer -\n",
        "            # add it to the list of trainable layers\n",
        "            # We don't need to check for biases -\n",
        "            # checking for weights is enough\n",
        "            if hasattr(self.layers[i], 'weights'):\n",
        "                self.trainable_layers.append(self.layers[i])\n",
        "\n",
        "        # Update loss object with trainable layers\n",
        "        self.loss.remember_trainable_layers(\n",
        "            self.trainable_layers\n",
        "        )\n",
        "\n",
        "        # If output activation is Softmax and\n",
        "        # loss function is Categorical Cross-Entropy\n",
        "        # create an object of combined activation\n",
        "        # and loss function containing\n",
        "        # faster gradient calculation\n",
        "        if isinstance(self.layers[-1], Activation_Softmax) and isinstance(self.loss, Loss_CategoricalCrossentropy):\n",
        "            # Create an object of combined activation\n",
        "            # and loss functions\n",
        "            self.softmax_classifier_output = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
        "\n",
        "    # Train the model\n",
        "    def train(self, X, y, *, epochs=1, batch_size=None,\n",
        "              print_every=1, validation_data=None):\n",
        "\n",
        "        # Initialize accuracy object\n",
        "        self.accuracy.init(y)\n",
        "\n",
        "        # Default value if batch size is not being set\n",
        "        train_steps = 1\n",
        "\n",
        "        # If there is validation data passed,\n",
        "        # set default number of steps for validation as well\n",
        "        if validation_data is not None:\n",
        "            validation_steps = 1\n",
        "\n",
        "            # For better readability\n",
        "            X_val, y_val = validation_data\n",
        "\n",
        "        # Calculate number of steps\n",
        "        if batch_size is not None:\n",
        "            train_steps = len(X) // batch_size\n",
        "            # Dividing rounds down. If there are some remaining\n",
        "            # data but not a full batch, this won't include it\n",
        "            # Add `1` to include this not full batch\n",
        "            if train_steps * batch_size < len(X):\n",
        "                train_steps += 1\n",
        "\n",
        "            if validation_data is not None:\n",
        "                validation_steps = len(X_val) // batch_size\n",
        "\n",
        "                # Dividing rounds down. If there are some remaining\n",
        "                # data but nor full batch, this won't include it\n",
        "                # Add `1` to include this not full batch\n",
        "                if validation_steps * batch_size < len(X_val):\n",
        "                    validation_steps += 1\n",
        "\n",
        "\n",
        "        # Main training loop\n",
        "        for epoch in range(1, epochs+1):\n",
        "\n",
        "            # Print epoch number\n",
        "            print(f'epoch: {epoch}')\n",
        "\n",
        "            # Reset accumulated values in loss and accuracy objects\n",
        "            self.loss.new_pass()\n",
        "            self.accuracy.new_pass()\n",
        "\n",
        "            # Iterate over steps\n",
        "            for step in range(train_steps):\n",
        "\n",
        "                # If batch size is not set -\n",
        "                # train using one step and full dataset\n",
        "                if batch_size is None:\n",
        "                    batch_X = X\n",
        "                    batch_y = y\n",
        "\n",
        "                # Otherwise slice a batch\n",
        "                else:\n",
        "                    batch_X = X[step*batch_size:(step+1)*batch_size]\n",
        "                    batch_y = y[step*batch_size:(step+1)*batch_size]\n",
        "\n",
        "                # Perform the forward pass\n",
        "                output = self.forward(batch_X, training=True)\n",
        "\n",
        "                # Calculate loss\n",
        "                data_loss, regularization_loss = self.loss.calculate(output, batch_y, include_regularization=True)\n",
        "                loss = data_loss + regularization_loss\n",
        "\n",
        "                # Get predictions and calculate an accuracy\n",
        "                predictions = self.output_layer_activation.predictions(output)\n",
        "                accuracy = self.accuracy.calculate(predictions,\n",
        "                                                   batch_y)\n",
        "\n",
        "                # Perform backward pass\n",
        "                self.backward(output, batch_y)\n",
        "\n",
        "                # Optimize (update parameters)\n",
        "                self.optimizer.pre_update_params()\n",
        "                for layer in self.trainable_layers:\n",
        "                    self.optimizer.update_params(layer)\n",
        "                self.optimizer.post_update_params()\n",
        "\n",
        "\n",
        "                # Print a summary\n",
        "                if not step % print_every or step == train_steps - 1:\n",
        "                    print(f'step: {step}, ' +\n",
        "                          f'acc: {accuracy:.3f}, ' +\n",
        "                          f'loss: {loss:.3f} (' +\n",
        "                          f'data_loss: {data_loss:.3f}, ' +\n",
        "                          f'reg_loss: {regularization_loss:.3f}), ' +\n",
        "                          f'lr: {self.optimizer.current_learning_rate}')\n",
        "\n",
        "            # Get and print epoch loss and accuracy\n",
        "            epoch_data_loss, epoch_regularization_loss = self.loss.calculate_accumulated(include_regularization=True)\n",
        "            epoch_loss = epoch_data_loss + epoch_regularization_loss\n",
        "            epoch_accuracy = self.accuracy.calculate_accumulated()\n",
        "\n",
        "            print(f'training, ' +\n",
        "                  f'acc: {epoch_accuracy:.3f}, ' +\n",
        "                  f'loss: {epoch_loss:.3f} (' +\n",
        "                  f'data_loss: {epoch_data_loss:.3f}, ' +\n",
        "                  f'reg_loss: {epoch_regularization_loss:.3f}), ' +\n",
        "                  f'lr: {self.optimizer.current_learning_rate}')\n",
        "\n",
        "            # If there is the validation data\n",
        "            if validation_data is not None:\n",
        "\n",
        "                # Reset accumulated values in loss\n",
        "                # and accuracy objects\n",
        "                self.loss.new_pass()\n",
        "                self.accuracy.new_pass()\n",
        "\n",
        "                # Iterate over steps\n",
        "                for step in range(validation_steps):\n",
        "\n",
        "                    # If batch size is not set -\n",
        "                    # train using one step and full dataset\n",
        "                    if batch_size is None:\n",
        "                        batch_X = X_val\n",
        "                        batch_y = y_val\n",
        "\n",
        "\n",
        "                    # Otherwise slice a batch\n",
        "                    else:\n",
        "                        batch_X = X_val[\n",
        "                            step*batch_size:(step+1)*batch_size\n",
        "                        ]\n",
        "                        batch_y = y_val[\n",
        "                            step*batch_size:(step+1)*batch_size\n",
        "                        ]\n",
        "\n",
        "                    # Perform the forward pass\n",
        "                    output = self.forward(batch_X, training=False)\n",
        "\n",
        "                    # Calculate the loss\n",
        "                    self.loss.calculate(output, batch_y)\n",
        "\n",
        "                    # Get predictions and calculate an accuracy\n",
        "                    predictions = self.output_layer_activation.predictions(output)\n",
        "                    self.accuracy.calculate(predictions, batch_y)\n",
        "\n",
        "                # Get and print validation loss and accuracy\n",
        "                validation_loss = self.loss.calculate_accumulated()\n",
        "                validation_accuracy = self.accuracy.calculate_accumulated()\n",
        "\n",
        "                # Print a summary\n",
        "                print(f'validation, ' +\n",
        "                      f'acc: {validation_accuracy:.3f}, ' +\n",
        "                      f'loss: {validation_loss:.3f}')\n",
        "\n",
        "    # Performs forward pass\n",
        "    def forward(self, X, training):\n",
        "\n",
        "        # Call forward method on the input layer\n",
        "        # this will set the output property that\n",
        "        # the first layer in \"prev\" object is expecting\n",
        "        self.input_layer.forward(X, training)\n",
        "\n",
        "        # Call forward method of every object in a chain\n",
        "        # Pass output of the previous object as a parameter\n",
        "        for layer in self.layers:\n",
        "            layer.forward(layer.prev.output, training)\n",
        "\n",
        "        # \"layer\" is now the last object from the list,\n",
        "        # return its output\n",
        "        return layer.output\n",
        "\n",
        "\n",
        "    # Performs backward pass\n",
        "    def backward(self, output, y):\n",
        "\n",
        "        # If softmax classifier\n",
        "        if self.softmax_classifier_output is not None:\n",
        "            # First call backward method\n",
        "            # on the combined activation/loss\n",
        "            # this will set dinputs property\n",
        "            self.softmax_classifier_output.backward(output, y)\n",
        "\n",
        "            # Since we'll not call backward method of the last layer\n",
        "            # which is Softmax activation\n",
        "            # as we used combined activation/loss\n",
        "            # object, let's set dinputs in this object\n",
        "            self.layers[-1].dinputs = self.softmax_classifier_output.dinputs\n",
        "\n",
        "            # Call backward method going through\n",
        "            # all the objects but last\n",
        "            # in reversed order passing dinputs as a parameter\n",
        "            for layer in reversed(self.layers[:-1]):\n",
        "                layer.backward(layer.next.dinputs)\n",
        "\n",
        "            return\n",
        "\n",
        "        # First call backward method on the loss\n",
        "        # this will set dinputs property that the last\n",
        "        # layer will try to access shortly\n",
        "        self.loss.backward(output, y)\n",
        "\n",
        "        # Call backward method going through all the objects\n",
        "        # in reversed order passing dinputs as a parameter\n",
        "        for layer in reversed(self.layers):\n",
        "            layer.backward(layer.next.dinputs)\n",
        "\n",
        "    \n",
        "\n",
        "\n"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgFKztc1EYdj"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tLzU-KTeXnY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98428441-b844-4eeb-973c-ad3893b58693"
      },
      "source": [
        "model = Model()\n",
        "\n",
        "\n",
        "# Add layers\n",
        "model.add(Layer_Dense(X.shape[1], 30))\n",
        "model.add(Activation_ReLU())\n",
        "model.add(Layer_Dense(30, 100))\n",
        "model.add(Activation_ReLU())\n",
        "model.add(Layer_Dense(100, 729))\n",
        "model.add(Activation_Softmax())\n",
        "\n",
        "# Set loss, optimizer and accuracy objects\n",
        "model.set(\n",
        "    loss=Loss_CategoricalCrossentropy(),\n",
        "    optimizer=Optimizer_Adam(decay=1e-3),\n",
        "    accuracy=Accuracy_Categorical()\n",
        ")\n",
        "\n",
        "# Finalize the model\n",
        "model.finalize()\n",
        "\n",
        "# Train the model\n",
        "model.train(X, y_transform, validation_data=(X_test, y_test_transform),epochs=2, batch_size=64, print_every=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1\n",
            "step: 0, acc: 0.111, loss: 177.975 (data_loss: 177.975, reg_loss: 0.000), lr: 0.001\n",
            "step: 10, acc: 0.114, loss: 177.980 (data_loss: 177.980, reg_loss: 0.000), lr: 0.0009900990099009901\n",
            "step: 20, acc: 0.112, loss: 178.017 (data_loss: 178.017, reg_loss: 0.000), lr: 0.000980392156862745\n",
            "step: 30, acc: 0.105, loss: 178.101 (data_loss: 178.101, reg_loss: 0.000), lr: 0.0009708737864077671\n",
            "step: 40, acc: 0.102, loss: 178.341 (data_loss: 178.341, reg_loss: 0.000), lr: 0.0009615384615384615\n",
            "step: 50, acc: 0.116, loss: 178.566 (data_loss: 178.566, reg_loss: 0.000), lr: 0.0009523809523809524\n",
            "step: 60, acc: 0.108, loss: 179.861 (data_loss: 179.861, reg_loss: 0.000), lr: 0.0009433962264150942\n",
            "step: 70, acc: 0.112, loss: 179.912 (data_loss: 179.912, reg_loss: 0.000), lr: 0.0009345794392523365\n",
            "step: 80, acc: 0.113, loss: 179.628 (data_loss: 179.628, reg_loss: 0.000), lr: 0.0009259259259259259\n",
            "step: 90, acc: 0.106, loss: 179.825 (data_loss: 179.825, reg_loss: 0.000), lr: 0.0009174311926605504\n",
            "step: 100, acc: 0.111, loss: 179.910 (data_loss: 179.910, reg_loss: 0.000), lr: 0.0009090909090909091\n",
            "step: 110, acc: 0.113, loss: 179.817 (data_loss: 179.817, reg_loss: 0.000), lr: 0.0009009009009009008\n",
            "step: 120, acc: 0.107, loss: 180.108 (data_loss: 180.108, reg_loss: 0.000), lr: 0.0008928571428571428\n",
            "step: 130, acc: 0.104, loss: 180.811 (data_loss: 180.811, reg_loss: 0.000), lr: 0.0008849557522123895\n",
            "step: 140, acc: 0.114, loss: 180.266 (data_loss: 180.266, reg_loss: 0.000), lr: 0.0008771929824561404\n",
            "step: 150, acc: 0.107, loss: 180.484 (data_loss: 180.484, reg_loss: 0.000), lr: 0.0008695652173913045\n",
            "step: 160, acc: 0.112, loss: 180.346 (data_loss: 180.346, reg_loss: 0.000), lr: 0.0008620689655172415\n",
            "step: 170, acc: 0.112, loss: 179.952 (data_loss: 179.952, reg_loss: 0.000), lr: 0.0008547008547008548\n",
            "step: 180, acc: 0.105, loss: 180.862 (data_loss: 180.862, reg_loss: 0.000), lr: 0.0008474576271186442\n",
            "step: 190, acc: 0.109, loss: 180.845 (data_loss: 180.845, reg_loss: 0.000), lr: 0.0008403361344537816\n",
            "step: 200, acc: 0.109, loss: 180.443 (data_loss: 180.443, reg_loss: 0.000), lr: 0.0008333333333333334\n",
            "step: 210, acc: 0.124, loss: 180.015 (data_loss: 180.015, reg_loss: 0.000), lr: 0.0008264462809917356\n",
            "step: 220, acc: 0.103, loss: 181.484 (data_loss: 181.484, reg_loss: 0.000), lr: 0.000819672131147541\n",
            "step: 230, acc: 0.105, loss: 180.721 (data_loss: 180.721, reg_loss: 0.000), lr: 0.0008130081300813008\n",
            "step: 240, acc: 0.114, loss: 180.941 (data_loss: 180.941, reg_loss: 0.000), lr: 0.0008064516129032259\n",
            "step: 250, acc: 0.118, loss: 180.812 (data_loss: 180.812, reg_loss: 0.000), lr: 0.0008\n",
            "step: 260, acc: 0.110, loss: 181.617 (data_loss: 181.617, reg_loss: 0.000), lr: 0.0007936507936507937\n",
            "step: 270, acc: 0.108, loss: 181.445 (data_loss: 181.445, reg_loss: 0.000), lr: 0.0007874015748031496\n",
            "step: 280, acc: 0.117, loss: 180.394 (data_loss: 180.394, reg_loss: 0.000), lr: 0.00078125\n",
            "step: 290, acc: 0.121, loss: 180.946 (data_loss: 180.946, reg_loss: 0.000), lr: 0.0007751937984496124\n",
            "step: 300, acc: 0.108, loss: 181.238 (data_loss: 181.238, reg_loss: 0.000), lr: 0.0007692307692307692\n",
            "step: 310, acc: 0.109, loss: 187.632 (data_loss: 187.632, reg_loss: 0.000), lr: 0.0007633587786259542\n",
            "step: 320, acc: 0.115, loss: 185.532 (data_loss: 185.532, reg_loss: 0.000), lr: 0.0007575757575757576\n",
            "step: 330, acc: 0.113, loss: 184.467 (data_loss: 184.467, reg_loss: 0.000), lr: 0.0007518796992481202\n",
            "step: 340, acc: 0.117, loss: 183.324 (data_loss: 183.324, reg_loss: 0.000), lr: 0.0007462686567164178\n",
            "step: 350, acc: 0.118, loss: 184.968 (data_loss: 184.968, reg_loss: 0.000), lr: 0.0007407407407407407\n",
            "step: 360, acc: 0.105, loss: 185.046 (data_loss: 185.046, reg_loss: 0.000), lr: 0.0007352941176470589\n",
            "step: 370, acc: 0.110, loss: 187.736 (data_loss: 187.736, reg_loss: 0.000), lr: 0.0007299270072992701\n",
            "step: 380, acc: 0.105, loss: 186.705 (data_loss: 186.705, reg_loss: 0.000), lr: 0.0007246376811594204\n",
            "step: 390, acc: 0.118, loss: 185.661 (data_loss: 185.661, reg_loss: 0.000), lr: 0.0007194244604316547\n",
            "step: 400, acc: 0.114, loss: 188.510 (data_loss: 188.510, reg_loss: 0.000), lr: 0.0007142857142857143\n",
            "step: 410, acc: 0.112, loss: 184.942 (data_loss: 184.942, reg_loss: 0.000), lr: 0.0007092198581560283\n",
            "step: 420, acc: 0.105, loss: 188.246 (data_loss: 188.246, reg_loss: 0.000), lr: 0.0007042253521126761\n",
            "step: 430, acc: 0.105, loss: 184.359 (data_loss: 184.359, reg_loss: 0.000), lr: 0.0006993006993006994\n",
            "step: 440, acc: 0.117, loss: 185.602 (data_loss: 185.602, reg_loss: 0.000), lr: 0.0006944444444444445\n",
            "step: 450, acc: 0.109, loss: 183.021 (data_loss: 183.021, reg_loss: 0.000), lr: 0.0006896551724137932\n",
            "step: 460, acc: 0.114, loss: 189.564 (data_loss: 189.564, reg_loss: 0.000), lr: 0.0006849315068493151\n",
            "step: 470, acc: 0.107, loss: 187.531 (data_loss: 187.531, reg_loss: 0.000), lr: 0.0006802721088435374\n",
            "step: 480, acc: 0.110, loss: 189.455 (data_loss: 189.455, reg_loss: 0.000), lr: 0.0006756756756756757\n",
            "step: 490, acc: 0.108, loss: 189.100 (data_loss: 189.100, reg_loss: 0.000), lr: 0.0006711409395973155\n",
            "step: 500, acc: 0.108, loss: 190.066 (data_loss: 190.066, reg_loss: 0.000), lr: 0.0006666666666666666\n",
            "step: 510, acc: 0.105, loss: 190.265 (data_loss: 190.265, reg_loss: 0.000), lr: 0.0006622516556291392\n",
            "step: 520, acc: 0.119, loss: 187.584 (data_loss: 187.584, reg_loss: 0.000), lr: 0.0006578947368421054\n",
            "step: 530, acc: 0.119, loss: 187.092 (data_loss: 187.092, reg_loss: 0.000), lr: 0.0006535947712418301\n",
            "step: 540, acc: 0.114, loss: 186.808 (data_loss: 186.808, reg_loss: 0.000), lr: 0.0006493506493506494\n",
            "step: 550, acc: 0.113, loss: 187.689 (data_loss: 187.689, reg_loss: 0.000), lr: 0.0006451612903225806\n",
            "step: 560, acc: 0.117, loss: 186.424 (data_loss: 186.424, reg_loss: 0.000), lr: 0.000641025641025641\n",
            "step: 570, acc: 0.114, loss: 187.696 (data_loss: 187.696, reg_loss: 0.000), lr: 0.0006369426751592356\n",
            "step: 580, acc: 0.122, loss: 189.690 (data_loss: 189.690, reg_loss: 0.000), lr: 0.0006329113924050632\n",
            "step: 590, acc: 0.116, loss: 191.904 (data_loss: 191.904, reg_loss: 0.000), lr: 0.000628930817610063\n",
            "step: 600, acc: 0.108, loss: 199.027 (data_loss: 199.027, reg_loss: 0.000), lr: 0.000625\n",
            "step: 610, acc: 0.113, loss: 203.458 (data_loss: 203.458, reg_loss: 0.000), lr: 0.0006211180124223603\n",
            "step: 620, acc: 0.109, loss: 204.627 (data_loss: 204.627, reg_loss: 0.000), lr: 0.0006172839506172839\n",
            "step: 630, acc: 0.114, loss: 212.464 (data_loss: 212.464, reg_loss: 0.000), lr: 0.0006134969325153375\n",
            "step: 640, acc: 0.106, loss: 208.139 (data_loss: 208.139, reg_loss: 0.000), lr: 0.0006097560975609756\n",
            "step: 650, acc: 0.112, loss: 209.894 (data_loss: 209.894, reg_loss: 0.000), lr: 0.0006060606060606061\n",
            "step: 660, acc: 0.110, loss: 231.712 (data_loss: 231.712, reg_loss: 0.000), lr: 0.0006024096385542168\n",
            "step: 670, acc: 0.105, loss: 297.599 (data_loss: 297.599, reg_loss: 0.000), lr: 0.0005988023952095809\n",
            "step: 680, acc: 0.115, loss: 486.709 (data_loss: 486.709, reg_loss: 0.000), lr: 0.0005952380952380953\n",
            "step: 690, acc: 0.110, loss: 868.810 (data_loss: 868.810, reg_loss: 0.000), lr: 0.000591715976331361\n",
            "step: 700, acc: 0.114, loss: 1029.584 (data_loss: 1029.584, reg_loss: 0.000), lr: 0.000588235294117647\n",
            "step: 710, acc: 0.105, loss: 1033.404 (data_loss: 1033.404, reg_loss: 0.000), lr: 0.0005847953216374269\n",
            "step: 720, acc: 0.104, loss: 1028.414 (data_loss: 1028.414, reg_loss: 0.000), lr: 0.0005813953488372093\n",
            "step: 730, acc: 0.111, loss: 1026.143 (data_loss: 1026.143, reg_loss: 0.000), lr: 0.0005780346820809249\n",
            "step: 740, acc: 0.115, loss: 988.728 (data_loss: 988.728, reg_loss: 0.000), lr: 0.0005747126436781609\n",
            "step: 750, acc: 0.110, loss: 1022.625 (data_loss: 1022.625, reg_loss: 0.000), lr: 0.0005714285714285714\n",
            "step: 760, acc: 0.110, loss: 984.372 (data_loss: 984.372, reg_loss: 0.000), lr: 0.0005681818181818183\n",
            "step: 770, acc: 0.110, loss: 1002.376 (data_loss: 1002.376, reg_loss: 0.000), lr: 0.0005649717514124294\n",
            "step: 780, acc: 0.112, loss: 947.618 (data_loss: 947.618, reg_loss: 0.000), lr: 0.0005617977528089888\n",
            "step: 790, acc: 0.116, loss: 953.677 (data_loss: 953.677, reg_loss: 0.000), lr: 0.0005586592178770949\n",
            "step: 800, acc: 0.112, loss: 979.217 (data_loss: 979.217, reg_loss: 0.000), lr: 0.0005555555555555556\n",
            "step: 810, acc: 0.113, loss: 964.816 (data_loss: 964.816, reg_loss: 0.000), lr: 0.0005524861878453039\n",
            "step: 820, acc: 0.117, loss: 934.270 (data_loss: 934.270, reg_loss: 0.000), lr: 0.0005494505494505495\n",
            "step: 830, acc: 0.113, loss: 959.544 (data_loss: 959.544, reg_loss: 0.000), lr: 0.000546448087431694\n",
            "step: 840, acc: 0.106, loss: 977.833 (data_loss: 977.833, reg_loss: 0.000), lr: 0.0005434782608695653\n",
            "step: 850, acc: 0.111, loss: 986.179 (data_loss: 986.179, reg_loss: 0.000), lr: 0.0005405405405405404\n",
            "step: 860, acc: 0.111, loss: 963.963 (data_loss: 963.963, reg_loss: 0.000), lr: 0.0005376344086021506\n",
            "step: 870, acc: 0.113, loss: 969.084 (data_loss: 969.084, reg_loss: 0.000), lr: 0.00053475935828877\n",
            "step: 880, acc: 0.112, loss: 984.519 (data_loss: 984.519, reg_loss: 0.000), lr: 0.0005319148936170213\n",
            "step: 890, acc: 0.109, loss: 996.160 (data_loss: 996.160, reg_loss: 0.000), lr: 0.0005291005291005291\n",
            "step: 900, acc: 0.116, loss: 944.568 (data_loss: 944.568, reg_loss: 0.000), lr: 0.0005263157894736842\n",
            "step: 910, acc: 0.110, loss: 956.051 (data_loss: 956.051, reg_loss: 0.000), lr: 0.0005235602094240838\n",
            "step: 920, acc: 0.106, loss: 989.366 (data_loss: 989.366, reg_loss: 0.000), lr: 0.0005208333333333334\n",
            "step: 930, acc: 0.107, loss: 998.731 (data_loss: 998.731, reg_loss: 0.000), lr: 0.0005181347150259067\n",
            "step: 940, acc: 0.121, loss: 956.136 (data_loss: 956.136, reg_loss: 0.000), lr: 0.0005154639175257733\n",
            "step: 950, acc: 0.108, loss: 996.423 (data_loss: 996.423, reg_loss: 0.000), lr: 0.0005128205128205128\n",
            "step: 960, acc: 0.103, loss: 969.170 (data_loss: 969.170, reg_loss: 0.000), lr: 0.0005102040816326531\n",
            "step: 970, acc: 0.111, loss: 972.248 (data_loss: 972.248, reg_loss: 0.000), lr: 0.0005076142131979696\n",
            "step: 980, acc: 0.115, loss: 967.865 (data_loss: 967.865, reg_loss: 0.000), lr: 0.000505050505050505\n",
            "step: 990, acc: 0.109, loss: 961.352 (data_loss: 961.352, reg_loss: 0.000), lr: 0.0005025125628140703\n",
            "step: 1000, acc: 0.110, loss: 963.808 (data_loss: 963.808, reg_loss: 0.000), lr: 0.0005\n",
            "step: 1010, acc: 0.109, loss: 977.183 (data_loss: 977.183, reg_loss: 0.000), lr: 0.0004975124378109454\n",
            "step: 1020, acc: 0.110, loss: 975.265 (data_loss: 975.265, reg_loss: 0.000), lr: 0.0004950495049504951\n",
            "step: 1030, acc: 0.112, loss: 959.766 (data_loss: 959.766, reg_loss: 0.000), lr: 0.0004926108374384236\n",
            "step: 1040, acc: 0.112, loss: 949.939 (data_loss: 949.939, reg_loss: 0.000), lr: 0.0004901960784313725\n",
            "step: 1050, acc: 0.111, loss: 949.029 (data_loss: 949.029, reg_loss: 0.000), lr: 0.00048780487804878054\n",
            "step: 1060, acc: 0.111, loss: 955.028 (data_loss: 955.028, reg_loss: 0.000), lr: 0.00048543689320388353\n",
            "step: 1070, acc: 0.114, loss: 985.936 (data_loss: 985.936, reg_loss: 0.000), lr: 0.00048309178743961346\n",
            "step: 1080, acc: 0.115, loss: 978.093 (data_loss: 978.093, reg_loss: 0.000), lr: 0.00048076923076923074\n",
            "step: 1090, acc: 0.109, loss: 1012.804 (data_loss: 1012.804, reg_loss: 0.000), lr: 0.00047846889952153117\n",
            "step: 1100, acc: 0.118, loss: 978.340 (data_loss: 978.340, reg_loss: 0.000), lr: 0.0004761904761904762\n",
            "step: 1110, acc: 0.113, loss: 997.016 (data_loss: 997.016, reg_loss: 0.000), lr: 0.0004739336492890995\n",
            "step: 1120, acc: 0.110, loss: 987.659 (data_loss: 987.659, reg_loss: 0.000), lr: 0.0004716981132075471\n",
            "step: 1130, acc: 0.112, loss: 980.141 (data_loss: 980.141, reg_loss: 0.000), lr: 0.00046948356807511736\n",
            "step: 1140, acc: 0.118, loss: 958.512 (data_loss: 958.512, reg_loss: 0.000), lr: 0.00046728971962616824\n",
            "step: 1150, acc: 0.112, loss: 945.788 (data_loss: 945.788, reg_loss: 0.000), lr: 0.0004651162790697674\n",
            "step: 1160, acc: 0.117, loss: 983.907 (data_loss: 983.907, reg_loss: 0.000), lr: 0.0004629629629629629\n",
            "step: 1170, acc: 0.106, loss: 959.740 (data_loss: 959.740, reg_loss: 0.000), lr: 0.0004608294930875576\n",
            "step: 1180, acc: 0.114, loss: 1004.716 (data_loss: 1004.716, reg_loss: 0.000), lr: 0.0004587155963302753\n",
            "step: 1190, acc: 0.111, loss: 981.657 (data_loss: 981.657, reg_loss: 0.000), lr: 0.0004566210045662101\n",
            "step: 1200, acc: 0.118, loss: 991.080 (data_loss: 991.080, reg_loss: 0.000), lr: 0.00045454545454545455\n",
            "step: 1210, acc: 0.121, loss: 985.170 (data_loss: 985.170, reg_loss: 0.000), lr: 0.00045248868778280545\n",
            "step: 1220, acc: 0.105, loss: 993.762 (data_loss: 993.762, reg_loss: 0.000), lr: 0.0004504504504504505\n",
            "step: 1230, acc: 0.116, loss: 986.983 (data_loss: 986.983, reg_loss: 0.000), lr: 0.0004484304932735426\n",
            "step: 1240, acc: 0.103, loss: 1005.607 (data_loss: 1005.607, reg_loss: 0.000), lr: 0.0004464285714285714\n",
            "step: 1250, acc: 0.113, loss: 980.377 (data_loss: 980.377, reg_loss: 0.000), lr: 0.0004444444444444444\n",
            "step: 1260, acc: 0.109, loss: 968.944 (data_loss: 968.944, reg_loss: 0.000), lr: 0.00044247787610619474\n",
            "step: 1270, acc: 0.117, loss: 999.872 (data_loss: 999.872, reg_loss: 0.000), lr: 0.00044052863436123345\n",
            "step: 1280, acc: 0.107, loss: 979.043 (data_loss: 979.043, reg_loss: 0.000), lr: 0.0004385964912280702\n",
            "step: 1290, acc: 0.110, loss: 988.007 (data_loss: 988.007, reg_loss: 0.000), lr: 0.0004366812227074236\n",
            "step: 1300, acc: 0.107, loss: 1008.032 (data_loss: 1008.032, reg_loss: 0.000), lr: 0.00043478260869565224\n",
            "step: 1310, acc: 0.113, loss: 1004.838 (data_loss: 1004.838, reg_loss: 0.000), lr: 0.0004329004329004329\n",
            "step: 1320, acc: 0.110, loss: 989.562 (data_loss: 989.562, reg_loss: 0.000), lr: 0.0004310344827586207\n",
            "step: 1330, acc: 0.115, loss: 980.605 (data_loss: 980.605, reg_loss: 0.000), lr: 0.0004291845493562232\n",
            "step: 1340, acc: 0.108, loss: 1002.401 (data_loss: 1002.401, reg_loss: 0.000), lr: 0.0004273504273504274\n",
            "step: 1350, acc: 0.108, loss: 997.249 (data_loss: 997.249, reg_loss: 0.000), lr: 0.000425531914893617\n",
            "step: 1360, acc: 0.109, loss: 968.114 (data_loss: 968.114, reg_loss: 0.000), lr: 0.00042372881355932197\n",
            "step: 1370, acc: 0.102, loss: 1004.752 (data_loss: 1004.752, reg_loss: 0.000), lr: 0.00042194092827004215\n",
            "step: 1380, acc: 0.115, loss: 980.312 (data_loss: 980.312, reg_loss: 0.000), lr: 0.0004201680672268908\n",
            "step: 1390, acc: 0.112, loss: 989.581 (data_loss: 989.581, reg_loss: 0.000), lr: 0.00041841004184100416\n",
            "step: 1400, acc: 0.117, loss: 985.993 (data_loss: 985.993, reg_loss: 0.000), lr: 0.00041666666666666664\n",
            "step: 1410, acc: 0.106, loss: 988.386 (data_loss: 988.386, reg_loss: 0.000), lr: 0.0004149377593360996\n",
            "step: 1420, acc: 0.108, loss: 995.365 (data_loss: 995.365, reg_loss: 0.000), lr: 0.0004132231404958678\n",
            "step: 1430, acc: 0.113, loss: 1005.205 (data_loss: 1005.205, reg_loss: 0.000), lr: 0.00041152263374485607\n",
            "step: 1440, acc: 0.116, loss: 1001.044 (data_loss: 1001.044, reg_loss: 0.000), lr: 0.0004098360655737705\n",
            "step: 1450, acc: 0.114, loss: 1018.118 (data_loss: 1018.118, reg_loss: 0.000), lr: 0.0004081632653061224\n",
            "step: 1460, acc: 0.116, loss: 1011.861 (data_loss: 1011.861, reg_loss: 0.000), lr: 0.0004065040650406504\n",
            "step: 1470, acc: 0.114, loss: 967.422 (data_loss: 967.422, reg_loss: 0.000), lr: 0.00040485829959514174\n",
            "step: 1480, acc: 0.108, loss: 1022.373 (data_loss: 1022.373, reg_loss: 0.000), lr: 0.00040322580645161296\n",
            "step: 1490, acc: 0.119, loss: 987.252 (data_loss: 987.252, reg_loss: 0.000), lr: 0.0004016064257028112\n",
            "step: 1500, acc: 0.110, loss: 974.220 (data_loss: 974.220, reg_loss: 0.000), lr: 0.0004\n",
            "step: 1510, acc: 0.103, loss: 1002.410 (data_loss: 1002.410, reg_loss: 0.000), lr: 0.0003984063745019921\n",
            "step: 1520, acc: 0.112, loss: 996.453 (data_loss: 996.453, reg_loss: 0.000), lr: 0.0003968253968253968\n",
            "step: 1530, acc: 0.114, loss: 1011.365 (data_loss: 1011.365, reg_loss: 0.000), lr: 0.00039525691699604737\n",
            "step: 1540, acc: 0.116, loss: 1000.518 (data_loss: 1000.518, reg_loss: 0.000), lr: 0.0003937007874015748\n",
            "step: 1550, acc: 0.115, loss: 1008.785 (data_loss: 1008.785, reg_loss: 0.000), lr: 0.0003921568627450981\n",
            "step: 1560, acc: 0.116, loss: 998.583 (data_loss: 998.583, reg_loss: 0.000), lr: 0.000390625\n",
            "step: 1570, acc: 0.107, loss: 1007.008 (data_loss: 1007.008, reg_loss: 0.000), lr: 0.00038910505836575873\n",
            "step: 1580, acc: 0.110, loss: 1012.124 (data_loss: 1012.124, reg_loss: 0.000), lr: 0.0003875968992248062\n",
            "step: 1590, acc: 0.121, loss: 974.385 (data_loss: 974.385, reg_loss: 0.000), lr: 0.0003861003861003861\n",
            "step: 1600, acc: 0.115, loss: 1001.615 (data_loss: 1001.615, reg_loss: 0.000), lr: 0.0003846153846153846\n",
            "step: 1610, acc: 0.117, loss: 1012.856 (data_loss: 1012.856, reg_loss: 0.000), lr: 0.00038314176245210724\n",
            "step: 1620, acc: 0.113, loss: 994.350 (data_loss: 994.350, reg_loss: 0.000), lr: 0.0003816793893129771\n",
            "step: 1630, acc: 0.110, loss: 1023.659 (data_loss: 1023.659, reg_loss: 0.000), lr: 0.0003802281368821293\n",
            "step: 1640, acc: 0.109, loss: 1001.280 (data_loss: 1001.280, reg_loss: 0.000), lr: 0.0003787878787878788\n",
            "step: 1650, acc: 0.109, loss: 1003.960 (data_loss: 1003.960, reg_loss: 0.000), lr: 0.0003773584905660377\n",
            "step: 1660, acc: 0.105, loss: 1005.769 (data_loss: 1005.769, reg_loss: 0.000), lr: 0.0003759398496240601\n",
            "step: 1670, acc: 0.110, loss: 1006.665 (data_loss: 1006.665, reg_loss: 0.000), lr: 0.0003745318352059925\n",
            "step: 1680, acc: 0.117, loss: 979.239 (data_loss: 979.239, reg_loss: 0.000), lr: 0.00037313432835820896\n",
            "step: 1690, acc: 0.114, loss: 980.918 (data_loss: 980.918, reg_loss: 0.000), lr: 0.00037174721189591083\n",
            "step: 1700, acc: 0.119, loss: 983.944 (data_loss: 983.944, reg_loss: 0.000), lr: 0.00037037037037037035\n",
            "step: 1710, acc: 0.109, loss: 987.512 (data_loss: 987.512, reg_loss: 0.000), lr: 0.00036900369003690036\n",
            "step: 1720, acc: 0.119, loss: 978.902 (data_loss: 978.902, reg_loss: 0.000), lr: 0.00036764705882352946\n",
            "step: 1730, acc: 0.114, loss: 1005.820 (data_loss: 1005.820, reg_loss: 0.000), lr: 0.0003663003663003663\n",
            "step: 1740, acc: 0.116, loss: 986.494 (data_loss: 986.494, reg_loss: 0.000), lr: 0.00036496350364963507\n",
            "step: 1750, acc: 0.110, loss: 1003.711 (data_loss: 1003.711, reg_loss: 0.000), lr: 0.00036363636363636367\n",
            "step: 1760, acc: 0.114, loss: 995.636 (data_loss: 995.636, reg_loss: 0.000), lr: 0.0003623188405797102\n",
            "step: 1770, acc: 0.112, loss: 997.111 (data_loss: 997.111, reg_loss: 0.000), lr: 0.0003610108303249098\n",
            "step: 1780, acc: 0.109, loss: 981.646 (data_loss: 981.646, reg_loss: 0.000), lr: 0.00035971223021582735\n",
            "step: 1790, acc: 0.115, loss: 1016.892 (data_loss: 1016.892, reg_loss: 0.000), lr: 0.00035842293906810036\n",
            "step: 1800, acc: 0.111, loss: 991.079 (data_loss: 991.079, reg_loss: 0.000), lr: 0.00035714285714285714\n",
            "step: 1810, acc: 0.111, loss: 1012.022 (data_loss: 1012.022, reg_loss: 0.000), lr: 0.00035587188612099647\n",
            "step: 1820, acc: 0.116, loss: 989.094 (data_loss: 989.094, reg_loss: 0.000), lr: 0.00035460992907801415\n",
            "step: 1830, acc: 0.118, loss: 1012.394 (data_loss: 1012.394, reg_loss: 0.000), lr: 0.000353356890459364\n",
            "step: 1840, acc: 0.116, loss: 981.731 (data_loss: 981.731, reg_loss: 0.000), lr: 0.00035211267605633805\n",
            "step: 1850, acc: 0.113, loss: 979.223 (data_loss: 979.223, reg_loss: 0.000), lr: 0.0003508771929824561\n",
            "step: 1860, acc: 0.114, loss: 1002.688 (data_loss: 1002.688, reg_loss: 0.000), lr: 0.00034965034965034965\n",
            "step: 1870, acc: 0.110, loss: 1013.878 (data_loss: 1013.878, reg_loss: 0.000), lr: 0.0003484320557491289\n",
            "step: 1880, acc: 0.110, loss: 1004.920 (data_loss: 1004.920, reg_loss: 0.000), lr: 0.00034722222222222224\n",
            "step: 1890, acc: 0.110, loss: 983.448 (data_loss: 983.448, reg_loss: 0.000), lr: 0.00034602076124567473\n",
            "step: 1900, acc: 0.106, loss: 1012.508 (data_loss: 1012.508, reg_loss: 0.000), lr: 0.0003448275862068965\n",
            "step: 1910, acc: 0.107, loss: 1003.508 (data_loss: 1003.508, reg_loss: 0.000), lr: 0.0003436426116838488\n",
            "step: 1920, acc: 0.110, loss: 995.368 (data_loss: 995.368, reg_loss: 0.000), lr: 0.00034246575342465754\n",
            "step: 1930, acc: 0.104, loss: 1004.714 (data_loss: 1004.714, reg_loss: 0.000), lr: 0.0003412969283276451\n",
            "step: 1940, acc: 0.115, loss: 1000.507 (data_loss: 1000.507, reg_loss: 0.000), lr: 0.0003401360544217687\n",
            "step: 1950, acc: 0.112, loss: 997.510 (data_loss: 997.510, reg_loss: 0.000), lr: 0.0003389830508474576\n",
            "step: 1960, acc: 0.112, loss: 997.141 (data_loss: 997.141, reg_loss: 0.000), lr: 0.00033783783783783786\n",
            "step: 1970, acc: 0.117, loss: 1019.032 (data_loss: 1019.032, reg_loss: 0.000), lr: 0.0003367003367003367\n",
            "step: 1980, acc: 0.106, loss: 989.500 (data_loss: 989.500, reg_loss: 0.000), lr: 0.00033557046979865775\n",
            "step: 1990, acc: 0.120, loss: 988.386 (data_loss: 988.386, reg_loss: 0.000), lr: 0.00033444816053511704\n",
            "step: 2000, acc: 0.111, loss: 1007.698 (data_loss: 1007.698, reg_loss: 0.000), lr: 0.0003333333333333333\n",
            "step: 2010, acc: 0.111, loss: 979.573 (data_loss: 979.573, reg_loss: 0.000), lr: 0.0003322259136212624\n",
            "step: 2020, acc: 0.113, loss: 990.515 (data_loss: 990.515, reg_loss: 0.000), lr: 0.0003311258278145696\n",
            "step: 2030, acc: 0.115, loss: 1002.294 (data_loss: 1002.294, reg_loss: 0.000), lr: 0.00033003300330033004\n",
            "step: 2040, acc: 0.120, loss: 1015.630 (data_loss: 1015.630, reg_loss: 0.000), lr: 0.0003289473684210527\n",
            "step: 2050, acc: 0.117, loss: 999.762 (data_loss: 999.762, reg_loss: 0.000), lr: 0.00032786885245901645\n",
            "step: 2060, acc: 0.116, loss: 1012.024 (data_loss: 1012.024, reg_loss: 0.000), lr: 0.00032679738562091506\n",
            "step: 2070, acc: 0.113, loss: 1008.096 (data_loss: 1008.096, reg_loss: 0.000), lr: 0.0003257328990228013\n",
            "step: 2080, acc: 0.109, loss: 1034.438 (data_loss: 1034.438, reg_loss: 0.000), lr: 0.0003246753246753247\n",
            "step: 2090, acc: 0.110, loss: 997.557 (data_loss: 997.557, reg_loss: 0.000), lr: 0.0003236245954692557\n",
            "step: 2100, acc: 0.106, loss: 1011.849 (data_loss: 1011.849, reg_loss: 0.000), lr: 0.0003225806451612903\n",
            "step: 2110, acc: 0.121, loss: 1011.339 (data_loss: 1011.339, reg_loss: 0.000), lr: 0.0003215434083601286\n",
            "step: 2120, acc: 0.101, loss: 1016.379 (data_loss: 1016.379, reg_loss: 0.000), lr: 0.0003205128205128205\n",
            "step: 2130, acc: 0.103, loss: 1003.410 (data_loss: 1003.410, reg_loss: 0.000), lr: 0.0003194888178913738\n",
            "step: 2140, acc: 0.104, loss: 1016.589 (data_loss: 1016.589, reg_loss: 0.000), lr: 0.0003184713375796178\n",
            "step: 2150, acc: 0.112, loss: 984.615 (data_loss: 984.615, reg_loss: 0.000), lr: 0.00031746031746031746\n",
            "step: 2160, acc: 0.111, loss: 979.764 (data_loss: 979.764, reg_loss: 0.000), lr: 0.0003164556962025316\n",
            "step: 2170, acc: 0.111, loss: 1006.314 (data_loss: 1006.314, reg_loss: 0.000), lr: 0.0003154574132492113\n",
            "step: 2180, acc: 0.111, loss: 997.877 (data_loss: 997.877, reg_loss: 0.000), lr: 0.00031446540880503143\n",
            "step: 2190, acc: 0.120, loss: 991.553 (data_loss: 991.553, reg_loss: 0.000), lr: 0.00031347962382445143\n",
            "step: 2200, acc: 0.104, loss: 1002.409 (data_loss: 1002.409, reg_loss: 0.000), lr: 0.0003125\n",
            "step: 2210, acc: 0.113, loss: 985.481 (data_loss: 985.481, reg_loss: 0.000), lr: 0.0003115264797507788\n",
            "step: 2220, acc: 0.107, loss: 1004.789 (data_loss: 1004.789, reg_loss: 0.000), lr: 0.0003105590062111801\n",
            "step: 2230, acc: 0.109, loss: 1009.266 (data_loss: 1009.266, reg_loss: 0.000), lr: 0.00030959752321981426\n",
            "step: 2240, acc: 0.114, loss: 989.726 (data_loss: 989.726, reg_loss: 0.000), lr: 0.00030864197530864197\n",
            "step: 2250, acc: 0.110, loss: 984.568 (data_loss: 984.568, reg_loss: 0.000), lr: 0.0003076923076923077\n",
            "step: 2260, acc: 0.117, loss: 1002.370 (data_loss: 1002.370, reg_loss: 0.000), lr: 0.00030674846625766873\n",
            "step: 2270, acc: 0.112, loss: 1038.093 (data_loss: 1038.093, reg_loss: 0.000), lr: 0.0003058103975535168\n",
            "step: 2280, acc: 0.116, loss: 1028.016 (data_loss: 1028.016, reg_loss: 0.000), lr: 0.0003048780487804878\n",
            "step: 2290, acc: 0.112, loss: 1026.312 (data_loss: 1026.312, reg_loss: 0.000), lr: 0.000303951367781155\n",
            "step: 2300, acc: 0.108, loss: 999.302 (data_loss: 999.302, reg_loss: 0.000), lr: 0.000303030303030303\n",
            "step: 2310, acc: 0.114, loss: 992.970 (data_loss: 992.970, reg_loss: 0.000), lr: 0.00030211480362537764\n",
            "step: 2320, acc: 0.107, loss: 1009.622 (data_loss: 1009.622, reg_loss: 0.000), lr: 0.00030120481927710846\n",
            "step: 2330, acc: 0.113, loss: 987.591 (data_loss: 987.591, reg_loss: 0.000), lr: 0.0003003003003003003\n",
            "step: 2340, acc: 0.108, loss: 986.442 (data_loss: 986.442, reg_loss: 0.000), lr: 0.00029940119760479047\n",
            "step: 2350, acc: 0.108, loss: 1002.949 (data_loss: 1002.949, reg_loss: 0.000), lr: 0.00029850746268656717\n",
            "step: 2360, acc: 0.120, loss: 1004.413 (data_loss: 1004.413, reg_loss: 0.000), lr: 0.00029761904761904765\n",
            "step: 2370, acc: 0.113, loss: 1000.936 (data_loss: 1000.936, reg_loss: 0.000), lr: 0.0002967359050445104\n",
            "step: 2380, acc: 0.113, loss: 1016.779 (data_loss: 1016.779, reg_loss: 0.000), lr: 0.0002958579881656805\n",
            "step: 2390, acc: 0.107, loss: 987.413 (data_loss: 987.413, reg_loss: 0.000), lr: 0.0002949852507374631\n",
            "step: 2400, acc: 0.106, loss: 999.566 (data_loss: 999.566, reg_loss: 0.000), lr: 0.00029411764705882356\n",
            "step: 2410, acc: 0.112, loss: 994.063 (data_loss: 994.063, reg_loss: 0.000), lr: 0.00029325513196480933\n",
            "step: 2420, acc: 0.109, loss: 989.841 (data_loss: 989.841, reg_loss: 0.000), lr: 0.00029239766081871346\n",
            "step: 2430, acc: 0.106, loss: 958.623 (data_loss: 958.623, reg_loss: 0.000), lr: 0.0002915451895043731\n",
            "step: 2440, acc: 0.116, loss: 1006.248 (data_loss: 1006.248, reg_loss: 0.000), lr: 0.00029069767441860465\n",
            "step: 2450, acc: 0.117, loss: 989.192 (data_loss: 989.192, reg_loss: 0.000), lr: 0.0002898550724637681\n",
            "step: 2460, acc: 0.111, loss: 998.155 (data_loss: 998.155, reg_loss: 0.000), lr: 0.00028901734104046245\n",
            "step: 2470, acc: 0.108, loss: 986.946 (data_loss: 986.946, reg_loss: 0.000), lr: 0.00028818443804034583\n",
            "step: 2480, acc: 0.105, loss: 1032.555 (data_loss: 1032.555, reg_loss: 0.000), lr: 0.00028735632183908046\n",
            "step: 2490, acc: 0.114, loss: 1000.813 (data_loss: 1000.813, reg_loss: 0.000), lr: 0.00028653295128939826\n",
            "step: 2500, acc: 0.113, loss: 1000.777 (data_loss: 1000.777, reg_loss: 0.000), lr: 0.0002857142857142857\n",
            "step: 2510, acc: 0.122, loss: 996.218 (data_loss: 996.218, reg_loss: 0.000), lr: 0.00028490028490028494\n",
            "step: 2520, acc: 0.107, loss: 1011.397 (data_loss: 1011.397, reg_loss: 0.000), lr: 0.00028409090909090913\n",
            "step: 2530, acc: 0.111, loss: 973.877 (data_loss: 973.877, reg_loss: 0.000), lr: 0.00028328611898017\n",
            "step: 2540, acc: 0.111, loss: 993.450 (data_loss: 993.450, reg_loss: 0.000), lr: 0.0002824858757062147\n",
            "step: 2550, acc: 0.103, loss: 1012.942 (data_loss: 1012.942, reg_loss: 0.000), lr: 0.0002816901408450704\n",
            "step: 2560, acc: 0.107, loss: 987.953 (data_loss: 987.953, reg_loss: 0.000), lr: 0.0002808988764044944\n",
            "step: 2570, acc: 0.109, loss: 1014.446 (data_loss: 1014.446, reg_loss: 0.000), lr: 0.0002801120448179272\n",
            "step: 2580, acc: 0.115, loss: 971.809 (data_loss: 971.809, reg_loss: 0.000), lr: 0.00027932960893854746\n",
            "step: 2590, acc: 0.104, loss: 1000.404 (data_loss: 1000.404, reg_loss: 0.000), lr: 0.0002785515320334262\n",
            "step: 2600, acc: 0.123, loss: 1003.251 (data_loss: 1003.251, reg_loss: 0.000), lr: 0.0002777777777777778\n",
            "step: 2610, acc: 0.113, loss: 1006.937 (data_loss: 1006.937, reg_loss: 0.000), lr: 0.0002770083102493075\n",
            "step: 2620, acc: 0.108, loss: 1005.579 (data_loss: 1005.579, reg_loss: 0.000), lr: 0.00027624309392265195\n",
            "step: 2630, acc: 0.113, loss: 999.467 (data_loss: 999.467, reg_loss: 0.000), lr: 0.0002754820936639119\n",
            "step: 2640, acc: 0.115, loss: 1000.606 (data_loss: 1000.606, reg_loss: 0.000), lr: 0.0002747252747252747\n",
            "step: 2650, acc: 0.109, loss: 1000.957 (data_loss: 1000.957, reg_loss: 0.000), lr: 0.000273972602739726\n",
            "step: 2660, acc: 0.113, loss: 1020.617 (data_loss: 1020.617, reg_loss: 0.000), lr: 0.000273224043715847\n",
            "step: 2670, acc: 0.113, loss: 1000.693 (data_loss: 1000.693, reg_loss: 0.000), lr: 0.00027247956403269756\n",
            "step: 2680, acc: 0.104, loss: 1009.385 (data_loss: 1009.385, reg_loss: 0.000), lr: 0.0002717391304347826\n",
            "step: 2690, acc: 0.114, loss: 991.172 (data_loss: 991.172, reg_loss: 0.000), lr: 0.0002710027100271003\n",
            "step: 2700, acc: 0.121, loss: 995.061 (data_loss: 995.061, reg_loss: 0.000), lr: 0.0002702702702702702\n",
            "step: 2710, acc: 0.111, loss: 1013.184 (data_loss: 1013.184, reg_loss: 0.000), lr: 0.0002695417789757412\n",
            "step: 2720, acc: 0.110, loss: 1011.087 (data_loss: 1011.087, reg_loss: 0.000), lr: 0.00026881720430107527\n",
            "step: 2730, acc: 0.114, loss: 986.146 (data_loss: 986.146, reg_loss: 0.000), lr: 0.00026809651474530834\n",
            "step: 2740, acc: 0.113, loss: 976.281 (data_loss: 976.281, reg_loss: 0.000), lr: 0.000267379679144385\n",
            "step: 2750, acc: 0.108, loss: 992.458 (data_loss: 992.458, reg_loss: 0.000), lr: 0.0002666666666666667\n",
            "step: 2760, acc: 0.112, loss: 1002.496 (data_loss: 1002.496, reg_loss: 0.000), lr: 0.00026595744680851064\n",
            "step: 2770, acc: 0.118, loss: 1006.956 (data_loss: 1006.956, reg_loss: 0.000), lr: 0.00026525198938992045\n",
            "step: 2780, acc: 0.114, loss: 974.464 (data_loss: 974.464, reg_loss: 0.000), lr: 0.00026455026455026457\n",
            "step: 2790, acc: 0.113, loss: 1028.468 (data_loss: 1028.468, reg_loss: 0.000), lr: 0.0002638522427440633\n",
            "step: 2800, acc: 0.111, loss: 998.124 (data_loss: 998.124, reg_loss: 0.000), lr: 0.0002631578947368421\n",
            "step: 2810, acc: 0.111, loss: 997.994 (data_loss: 997.994, reg_loss: 0.000), lr: 0.00026246719160104987\n",
            "step: 2820, acc: 0.104, loss: 1020.733 (data_loss: 1020.733, reg_loss: 0.000), lr: 0.0002617801047120419\n",
            "step: 2830, acc: 0.115, loss: 1012.144 (data_loss: 1012.144, reg_loss: 0.000), lr: 0.0002610966057441253\n",
            "step: 2840, acc: 0.107, loss: 1008.872 (data_loss: 1008.872, reg_loss: 0.000), lr: 0.0002604166666666667\n",
            "step: 2850, acc: 0.109, loss: 1004.281 (data_loss: 1004.281, reg_loss: 0.000), lr: 0.00025974025974025974\n",
            "step: 2860, acc: 0.113, loss: 982.233 (data_loss: 982.233, reg_loss: 0.000), lr: 0.0002590673575129534\n",
            "step: 2870, acc: 0.110, loss: 1011.077 (data_loss: 1011.077, reg_loss: 0.000), lr: 0.00025839793281653745\n",
            "step: 2880, acc: 0.117, loss: 987.578 (data_loss: 987.578, reg_loss: 0.000), lr: 0.00025773195876288666\n",
            "step: 2890, acc: 0.107, loss: 1009.818 (data_loss: 1009.818, reg_loss: 0.000), lr: 0.0002570694087403599\n",
            "step: 2900, acc: 0.114, loss: 1006.578 (data_loss: 1006.578, reg_loss: 0.000), lr: 0.00025641025641025646\n",
            "step: 2910, acc: 0.110, loss: 1020.947 (data_loss: 1020.947, reg_loss: 0.000), lr: 0.00025575447570332484\n",
            "step: 2920, acc: 0.107, loss: 1029.879 (data_loss: 1029.879, reg_loss: 0.000), lr: 0.00025510204081632655\n",
            "step: 2930, acc: 0.109, loss: 1028.548 (data_loss: 1028.548, reg_loss: 0.000), lr: 0.0002544529262086514\n",
            "step: 2940, acc: 0.110, loss: 1018.286 (data_loss: 1018.286, reg_loss: 0.000), lr: 0.0002538071065989848\n",
            "step: 2950, acc: 0.110, loss: 997.122 (data_loss: 997.122, reg_loss: 0.000), lr: 0.0002531645569620253\n",
            "step: 2960, acc: 0.108, loss: 1020.766 (data_loss: 1020.766, reg_loss: 0.000), lr: 0.0002525252525252525\n",
            "step: 2970, acc: 0.110, loss: 981.618 (data_loss: 981.618, reg_loss: 0.000), lr: 0.00025188916876574307\n",
            "step: 2980, acc: 0.105, loss: 1019.364 (data_loss: 1019.364, reg_loss: 0.000), lr: 0.00025125628140703515\n",
            "step: 2990, acc: 0.112, loss: 1009.819 (data_loss: 1009.819, reg_loss: 0.000), lr: 0.0002506265664160401\n",
            "step: 3000, acc: 0.112, loss: 995.138 (data_loss: 995.138, reg_loss: 0.000), lr: 0.00025\n",
            "step: 3010, acc: 0.111, loss: 998.908 (data_loss: 998.908, reg_loss: 0.000), lr: 0.0002493765586034913\n",
            "step: 3020, acc: 0.114, loss: 1007.162 (data_loss: 1007.162, reg_loss: 0.000), lr: 0.0002487562189054727\n",
            "step: 3030, acc: 0.111, loss: 979.089 (data_loss: 979.089, reg_loss: 0.000), lr: 0.00024813895781637717\n",
            "step: 3040, acc: 0.111, loss: 998.868 (data_loss: 998.868, reg_loss: 0.000), lr: 0.00024752475247524753\n",
            "step: 3050, acc: 0.109, loss: 973.327 (data_loss: 973.327, reg_loss: 0.000), lr: 0.00024691358024691353\n",
            "step: 3060, acc: 0.107, loss: 1005.456 (data_loss: 1005.456, reg_loss: 0.000), lr: 0.0002463054187192118\n",
            "step: 3070, acc: 0.112, loss: 1007.175 (data_loss: 1007.175, reg_loss: 0.000), lr: 0.0002457002457002457\n",
            "step: 3080, acc: 0.109, loss: 971.063 (data_loss: 971.063, reg_loss: 0.000), lr: 0.00024509803921568627\n",
            "step: 3090, acc: 0.107, loss: 1016.321 (data_loss: 1016.321, reg_loss: 0.000), lr: 0.0002444987775061125\n",
            "step: 3100, acc: 0.111, loss: 974.298 (data_loss: 974.298, reg_loss: 0.000), lr: 0.00024390243902439027\n",
            "step: 3110, acc: 0.108, loss: 989.305 (data_loss: 989.305, reg_loss: 0.000), lr: 0.00024330900243309006\n",
            "step: 3120, acc: 0.106, loss: 1001.117 (data_loss: 1001.117, reg_loss: 0.000), lr: 0.00024271844660194176\n",
            "step: 3130, acc: 0.108, loss: 1008.083 (data_loss: 1008.083, reg_loss: 0.000), lr: 0.00024213075060532688\n",
            "step: 3140, acc: 0.108, loss: 987.231 (data_loss: 987.231, reg_loss: 0.000), lr: 0.00024154589371980673\n",
            "step: 3150, acc: 0.113, loss: 976.552 (data_loss: 976.552, reg_loss: 0.000), lr: 0.00024096385542168674\n",
            "step: 3160, acc: 0.111, loss: 1016.194 (data_loss: 1016.194, reg_loss: 0.000), lr: 0.00024038461538461537\n",
            "step: 3170, acc: 0.108, loss: 998.866 (data_loss: 998.866, reg_loss: 0.000), lr: 0.00023980815347721823\n",
            "step: 3180, acc: 0.110, loss: 987.130 (data_loss: 987.130, reg_loss: 0.000), lr: 0.00023923444976076558\n",
            "step: 3190, acc: 0.118, loss: 986.690 (data_loss: 986.690, reg_loss: 0.000), lr: 0.00023866348448687357\n",
            "step: 3200, acc: 0.106, loss: 981.564 (data_loss: 981.564, reg_loss: 0.000), lr: 0.0002380952380952381\n",
            "step: 3210, acc: 0.111, loss: 1001.498 (data_loss: 1001.498, reg_loss: 0.000), lr: 0.00023752969121140145\n",
            "step: 3220, acc: 0.111, loss: 982.603 (data_loss: 982.603, reg_loss: 0.000), lr: 0.00023696682464454974\n",
            "step: 3230, acc: 0.110, loss: 986.984 (data_loss: 986.984, reg_loss: 0.000), lr: 0.00023640661938534278\n",
            "step: 3240, acc: 0.115, loss: 990.275 (data_loss: 990.275, reg_loss: 0.000), lr: 0.00023584905660377356\n",
            "step: 3250, acc: 0.110, loss: 1003.473 (data_loss: 1003.473, reg_loss: 0.000), lr: 0.00023529411764705883\n",
            "step: 3260, acc: 0.107, loss: 994.586 (data_loss: 994.586, reg_loss: 0.000), lr: 0.00023474178403755868\n",
            "step: 3270, acc: 0.111, loss: 1004.052 (data_loss: 1004.052, reg_loss: 0.000), lr: 0.00023419203747072602\n",
            "step: 3280, acc: 0.109, loss: 997.063 (data_loss: 997.063, reg_loss: 0.000), lr: 0.00023364485981308412\n",
            "step: 3290, acc: 0.116, loss: 973.541 (data_loss: 973.541, reg_loss: 0.000), lr: 0.0002331002331002331\n",
            "step: 3300, acc: 0.116, loss: 1002.597 (data_loss: 1002.597, reg_loss: 0.000), lr: 0.0002325581395348837\n",
            "step: 3310, acc: 0.112, loss: 1009.675 (data_loss: 1009.675, reg_loss: 0.000), lr: 0.00023201856148491877\n",
            "step: 3320, acc: 0.117, loss: 992.041 (data_loss: 992.041, reg_loss: 0.000), lr: 0.00023148148148148146\n",
            "step: 3330, acc: 0.117, loss: 993.286 (data_loss: 993.286, reg_loss: 0.000), lr: 0.00023094688221709007\n",
            "step: 3340, acc: 0.118, loss: 969.941 (data_loss: 969.941, reg_loss: 0.000), lr: 0.0002304147465437788\n",
            "step: 3350, acc: 0.113, loss: 1004.253 (data_loss: 1004.253, reg_loss: 0.000), lr: 0.00022988505747126439\n",
            "step: 3360, acc: 0.111, loss: 998.677 (data_loss: 998.677, reg_loss: 0.000), lr: 0.00022935779816513765\n",
            "step: 3370, acc: 0.112, loss: 987.483 (data_loss: 987.483, reg_loss: 0.000), lr: 0.00022883295194508008\n",
            "step: 3380, acc: 0.105, loss: 990.925 (data_loss: 990.925, reg_loss: 0.000), lr: 0.00022831050228310504\n",
            "step: 3390, acc: 0.114, loss: 993.766 (data_loss: 993.766, reg_loss: 0.000), lr: 0.0002277904328018223\n",
            "step: 3400, acc: 0.116, loss: 995.225 (data_loss: 995.225, reg_loss: 0.000), lr: 0.00022727272727272727\n",
            "step: 3410, acc: 0.107, loss: 993.998 (data_loss: 993.998, reg_loss: 0.000), lr: 0.00022675736961451246\n",
            "step: 3420, acc: 0.114, loss: 1004.103 (data_loss: 1004.103, reg_loss: 0.000), lr: 0.00022624434389140272\n",
            "step: 3430, acc: 0.107, loss: 1027.105 (data_loss: 1027.105, reg_loss: 0.000), lr: 0.00022573363431151246\n",
            "step: 3440, acc: 0.114, loss: 1002.117 (data_loss: 1002.117, reg_loss: 0.000), lr: 0.00022522522522522526\n",
            "step: 3450, acc: 0.111, loss: 976.034 (data_loss: 976.034, reg_loss: 0.000), lr: 0.00022471910112359551\n",
            "step: 3460, acc: 0.106, loss: 983.795 (data_loss: 983.795, reg_loss: 0.000), lr: 0.0002242152466367713\n",
            "step: 3470, acc: 0.113, loss: 991.047 (data_loss: 991.047, reg_loss: 0.000), lr: 0.00022371364653243846\n",
            "step: 3480, acc: 0.108, loss: 1007.057 (data_loss: 1007.057, reg_loss: 0.000), lr: 0.0002232142857142857\n",
            "step: 3490, acc: 0.106, loss: 998.733 (data_loss: 998.733, reg_loss: 0.000), lr: 0.00022271714922048996\n",
            "step: 3500, acc: 0.111, loss: 987.275 (data_loss: 987.275, reg_loss: 0.000), lr: 0.0002222222222222222\n",
            "step: 3510, acc: 0.110, loss: 967.249 (data_loss: 967.249, reg_loss: 0.000), lr: 0.00022172949002217298\n",
            "step: 3520, acc: 0.116, loss: 985.504 (data_loss: 985.504, reg_loss: 0.000), lr: 0.00022123893805309737\n",
            "step: 3530, acc: 0.115, loss: 973.671 (data_loss: 973.671, reg_loss: 0.000), lr: 0.0002207505518763797\n"
          ]
        }
      ]
    }
  ]
}